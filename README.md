## CourseProject script setup instruction for Reproducing paper "Generating Semantic Annotations for Frequent Patterns with Context Analysis" 

1. Overview of the github CourseProject 

	There are three folders, Datasets, PythonCodes and JupyterNoteBookDemo, in the repository. The PythonCodes and the JupyterNotebookDemo contain the same scripts but different file formats (py file and ipynb file). Within each of these two script folders, there are 6 folders giving the execution order for reproducing the paper using the DBLP dataset (paper section 5.1). All datasets imported and generated using the scripts are located in the Datasets Folder. The input and output paths of these Datasets files need to be changed if downloaded to your local computer. Except for the raw dataset "dblp50000.xml", all other datasets are generated by the scripts.

	In addition, the final report, the link (https://mediaspace.illinois.edu/media/t/1_0x0ykbbk) to the video demo and the powerpoint slides of paper review and project introduction are also in the CourseProject repository. 
	
2. Script running instruction

	2.1. Parse the raw data (Folder 1. RawDataParsing)
		Download "dblp50000.xml" and run script "DBLP_raw_data_parsing.py" or "DBLP_raw_data_parsing.ipynb". This generates the dataset "DBLP2000.csv". 

	2.2. Build the Context Units Space (Folder 2. ContextModeling)

	2.2.1. Find closed Frequent Pattern (FP) for Authors using FPgrowth algorithm in MLXtend Lib. 
		Import "DBLP2000.csv" and run script "Author_FP_mining.py" or "Author_FP_mining.ipynb". This generate the dataset "authorsFP2000_with_index.csv", which contains 14 closed FPs of authors and their transaction index in "DBLP2000.csv" (e.g. author "Edwin R. Hancock" is a closed FP and it showed in the 839th, 1119th, 1127th, 1204th and 1576th row of DBLP2000, its transaction index list is [839, 1119, 1127, 1204, 1576] ).  
	
	2.2.2. Preprocess DBLP titles
		Import "DBLP2000.csv" and run script "DBPL_preprocessing_titles.py" or "DBPL_preprocessing_titles.ipynb". This generates the dataset "DBLP2000_preprocessed_titles.txt". In this step, stop words are removed and the titles are stemmed. 

	2.2.3. Find Title sequential Pattern using PrefixSpan algorithm in PySpark
		Import "DBLP2000_preprocessed_titles.txt" and run "titles_seqPattern_mining.py" to find closed sequential frequent patterns from titles of DBLP2000. I had issue with configuration of Spark in Jupyter Notebook environment therefore no corresponding "ipynb" file was generated. However, the python script was executed successfully in the windows cmd of my laptop. "titles_seqPattern_mining.py" generats an output folder containing the pattern file "part-00000". Set the "part-00000" file to txt format. 
		Import "part-00000.txt" and run "Title_sequentialFP_processing.py" or "Title_sequentialFP_processing.ipynb" to generate the cleaned dataset "titlesFP2000.csv".

	2.2.4. Find transaction index of title sequential patterns
		Import "titlesFP2000.csv" and run "Find_transaction_index_of_title_FPs.py" or "Find_transaction_index_of_title_FPs.ipynb". This generates "titlesFP2000_with_index.csv", which adds the list of transaction index to each title pattern.

	2.2.5. Reduce title FP redundancy by microclustering (hierarchical clustering)
		Import "titlesFP2000_with_index.csv" and run "Hierarchical_clustering_titleFPs2000.py" or "Hierarchical_clustering_titleFPs2000.ipynb". This script applys hierarchical clustering with Jaccard Distance defined per paper and clusters 1912 title sequential patterns into 166 clusters and choose the most frequent pattern in each cluster as the "centroid" pattern to further build the contex units space. This script generates "titlesFP2000_final.csv".

	2.2.6. Combine author FPs and title FPs to build the context units space
		Import 'authorsFP2000_with_index.csv' and 'titlesFP2000_final.csv' and run "DBLP2000_context_units_with_transaction_index.py" or "DBLP2000_context_units_with_transaction_index.ipynb" to generate the final context units dataset "DBLP2000_context_units.csv".

	2.3. Define given frequent patterns using context units defined above (Folder 3. PatternDefinition)

	2.3.1. Build weight vectors of FPs in the context units space
		Import ""DBPL2000_context_units.csv" and "DBLP2000.csv" and run "Weighting_function.py" or "Weighting_function.ipynb" to generate "Context_units_weights.csv". This script generates context vectors for all context units defined in 2.2 and builds a weight matrix between pairwised context FPs. Each element of the matrix is the Mutual Information score between the context unit pair per definition in the paper. 

	2.3.2. Annotate the given FP (e.g. an author) by context units with highest weights
		Import "Context_units_weights.csv" and run "Defining_pattern_with_context_units.py" or "Defining_pattern_with_context_units.ipynb". In this step, we pick an author from the author FPs and rank the weights of its context vector. The context units with top 5 weights are selected as the definition of this author and are saved in "author_annotation_example1.csv". Similarly, we pick a title from the title FPs and rank the weights of its context vector, and save the context units with top 5 weights in "title_annotation_example1.csv".

	2.4. Find representative titles of the given pattern (Folder 4. RepresentativeTitles2Pattern)

		Import "DBLP2000_context_units.csv", "DBLP2000.csv" and "Context_units_weights.csv" and run script "Find_representative_titles_to_pattern.py" or "Find_representative_titles_to_pattern.ipynb". This script first generates the weight matrix of transactions (titles) in the context units space as the dataset "transaction_weights.csv", and then compute the cosine similarity between the transaction weight vectors and the given pattern weight vector (e.g. the same author and title chosen in 2.3.2). The similarity matrix of transaction to author FPs is saved as "similarity_scores_of_transaction_to_author.csv". The similarity matrix of transaction to title FPs is saved as "similarity_scores_of_transaction_to_title.csv". This step generates the top 5 representative titles with highest similarity scores to author and to title pattern as dataset "rep_titles_author_example1.csv" and "rep_titles_title_example1.csv", respectively.

	2.5. Find synonyms of the given pattern (Folder 5. Synonyms2Pattern)

		Import "Context_units_weights.csv" and run  "Find_synonyms_of_pattern.py" or "Find_synonyms_of_pattern.ipynb" to compute the cosine similarity between the candidate patterns of similarity (e.g. all closed frequent patterns of authors) and the given pattern (e.g. the same author and title chosen in 2.3.2). Select the authors with highest 5 similarity scores as the synonyms of the given author/title other than the author/title itself. This gives output 2 datasets for synonyms of author pattern: "coauthor_to_author_example1.csv", "syn_titles_to_author_example1.csv" and 2 datasets for synonyms of title pattern: "syn_titles_to_title_example1.csv" and "syn_authors_to_title_example1.csv".

	2.6. A final display of the context annotation of the given pattern (Folder 6. ContextAnnotation)

		Finally, use the output datasets generated in step 2.4, 2.5 and 2.6, and run "Author_context_annotation_example1.py" (or "Author_context_annotation_example1.ipynb") and "Title_context_annotation_example1.py" (or "Title_context_annotation_example1.ipnb") to build two examples of context annotation for an given author pattern and an given title pattern respectivley. This step fullfills the two experiments in paper section 5.1.